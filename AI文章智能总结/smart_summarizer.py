#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½AIæ–‡ç« æ€»ç»“ç³»ç»Ÿ
åœ¨AIæ€»ç»“æ—¶å®æ—¶è·å–ç½‘é¡µå†…å®¹ï¼Œè€Œä¸æ˜¯é¢„å…ˆçˆ¬å–
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Optional
import os
import sys

# æ·»åŠ çˆ¬è™«æ¨¡å—è·¯å¾„
sys.path.append('../çˆ¬å–AIå’¨è¯¢')

class SmartWebReader:
    """æ™ºèƒ½ç½‘é¡µè¯»å–å™¨"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://ai-bot.cn/',
        })

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('smart_summarizer.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_page_content(self, url: str, max_retries: int = 3) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except requests.RequestException as e:
                self.logger.warning(f"è·å–é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    self.logger.error(f"è·å–é¡µé¢æœ€ç»ˆå¤±è´¥: {url}")
                    return None

    def extract_article_content(self, html_content: str, url: str) -> str:
        """ä»HTMLä¸­æå–æ–‡ç« å†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # é’ˆå¯¹AIå·¥å…·é›†ç½‘ç«™ä¼˜åŒ–çš„å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            '.entry-content',           # WordPressæ ‡å‡†å†…å®¹åŒºåŸŸ
            '.post-content',            # æ–‡ç« å†…å®¹åŒºåŸŸ
            '.article-content',         # æ–‡ç« å†…å®¹
            '.content',                 # é€šç”¨å†…å®¹åŒºåŸŸ
            'article .content',         # æ–‡ç« æ ‡ç­¾å†…çš„å†…å®¹
            '.main-content',            # ä¸»è¦å†…å®¹åŒºåŸŸ
            '.post-body',               # æ–‡ç« ä¸»ä½“
            '.single-content',          # å•é¡µå†…å®¹
            '[class*="content"]',       # åŒ…å«contentçš„ç±»å
            'main article',             # ä¸»è¦æ–‡ç« åŒºåŸŸ
            '.wp-content'               # WordPresså†…å®¹
        ]

        content = ''
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for unwanted in content_elem(["script", "style", "nav", "footer", "header", ".sidebar", ".related", ".comments", ".navigation"]):
                    unwanted.decompose()

                # æå–æ–‡æœ¬å†…å®¹
                content = content_elem.get_text(separator='\n', strip=True)

                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
                content = '\n'.join(line.strip() for line in content.split('\n') if line.strip())

                if len(content) > 200:  # ç¡®ä¿å†…å®¹æœ‰è¶³å¤Ÿé•¿åº¦
                    break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å†…å®¹ï¼Œå°è¯•ä»æ•´ä¸ªé¡µé¢æå–
        if not content or len(content) < 100:
            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            for unwanted in soup(["script", "style", "nav", "footer", "header", ".sidebar", ".menu", ".navigation"]):
                unwanted.decompose()

            # å°è¯•ä»bodyæˆ–mainæ ‡ç­¾æå–
            body_content = soup.find('body') or soup.find('main')
            if body_content:
                content = body_content.get_text(separator='\n', strip=True)
                content = '\n'.join(line.strip() for line in content.split('\n') if line.strip())

        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿
        if len(content) > 4000:
            content = content[:4000] + "...[å†…å®¹å·²æˆªæ–­]"

        return content if content else "æ— æ³•è·å–æ–‡ç« è¯¦ç»†å†…å®¹"

    def read_article_realtime(self, article_url: str) -> str:
        """å®æ—¶è¯»å–æ–‡ç« å†…å®¹"""
        self.logger.info(f"å®æ—¶è¯»å–æ–‡ç« : {article_url}")

        html_content = self.get_page_content(article_url)
        if not html_content:
            return "æ— æ³•è·å–æ–‡ç« å†…å®¹"

        content = self.extract_article_content(html_content, article_url)
        self.logger.info(f"æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")

        return content

class FridayAIClient:
    """ç¾å›¢Fridayå¤§æ¨¡å‹å®¢æˆ·ç«¯"""

    def __init__(self, app_id: str):
        self.app_id = app_id
        self.base_url = "https://aigc.sankuai.com/v1/openai/native/chat/completions"
        self.headers = {
            'Authorization': f'Bearer {app_id}',
            'Content-Type': 'application/json'
        }

        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)

    def create_summary_prompt(self, article_data: Dict, realtime_content: str) -> str:
        """åˆ›å»ºæ–‡ç« æ€»ç»“çš„æç¤ºè¯"""
        prompt = f"""è¯·å¯¹ä»¥ä¸‹AIå·¥å…·/é¡¹ç›®æ–‡ç« è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ€»ç»“ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

åŸºæœ¬ä¿¡æ¯ï¼š
- æ–‡ç« æ ‡é¢˜ï¼š{article_data.get('title', '')}
- æ–‡ç« åˆ†ç±»ï¼š{article_data.get('category', '')}
- å‘å¸ƒæ—¶é—´ï¼š{article_data.get('publish_time', '')}
- æ–‡ç« é“¾æ¥ï¼š{article_data.get('url', '')}
- ç®€è¦æè¿°ï¼š{article_data.get('description', '')}

å®æ—¶è·å–çš„å®Œæ•´æ–‡ç« å†…å®¹ï¼š
{realtime_content}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ€»ç»“ï¼š

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½
[ç”¨1-2å¥è¯æ¦‚æ‹¬è¿™ä¸ªAIå·¥å…·/é¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½å’Œä»·å€¼]

## ğŸ”§ ä¸»è¦ç‰¹ç‚¹
[åˆ—å‡º3-5ä¸ªä¸»è¦ç‰¹ç‚¹æˆ–äº®ç‚¹ï¼Œæ¯ä¸ªç‰¹ç‚¹ç”¨ä¸€è¡Œç®€æ´æè¿°]

## ğŸ·ï¸ åº”ç”¨åœºæ™¯
[æè¿°é€‚ç”¨çš„å…·ä½“åº”ç”¨åœºæ™¯å’Œç›®æ ‡ç”¨æˆ·ç¾¤ä½“]

## ğŸ’¡ åˆ›æ–°ç‚¹
[æŒ‡å‡ºç›¸æ¯”åŒç±»äº§å“çš„åˆ›æ–°ä¹‹å¤„æˆ–ç‹¬ç‰¹ä¼˜åŠ¿]

## ğŸ“Š å®ç”¨æ€§è¯„ä¼°
[ä»æŠ€æœ¯æˆç†Ÿåº¦ã€æ˜“ç”¨æ€§ã€å®ç”¨ä»·å€¼ç­‰è§’åº¦ç»™å‡ºç®€è¦è¯„ä¼°]

è¯·ç¡®ä¿æ€»ç»“å†…å®¹ï¼š
1. ä¸“ä¸šå‡†ç¡®ï¼Œçªå‡ºæŠ€æœ¯ç‰¹ç‚¹
2. ç®€æ´æ˜äº†ï¼Œæ¯éƒ¨åˆ†æ§åˆ¶åœ¨50å­—ä»¥å†…
3. å®¢è§‚ä¸­æ€§ï¼Œé¿å…è¿‡åº¦è¥é”€è¯­è¨€
4. çªå‡ºå®ç”¨ä»·å€¼å’Œåº”ç”¨å‰æ™¯
5. åŸºäºå®æ—¶è·å–çš„å®Œæ•´å†…å®¹è¿›è¡Œåˆ†æ

å¦‚æœå®æ—¶å†…å®¹è·å–å¤±è´¥ï¼Œè¯·åŸºäºåŸºæœ¬ä¿¡æ¯è¿›è¡Œåˆç†æ¨æµ‹ï¼Œå¹¶æ ‡æ³¨"[åŸºäºåŸºæœ¬ä¿¡æ¯æ¨æµ‹]"ã€‚"""

        return prompt

    def call_friday_api(self, prompt: str, max_retries: int = 3) -> Optional[str]:
        """è°ƒç”¨Friday APIè·å–æ€»ç»“"""
        payload = {
            "model": "LongCat-Large-32K-Chat",
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": False,
            "temperature": 0.7,
            "max_tokens": 1000
        }

        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.base_url,
                    headers=self.headers,
                    json=payload,
                    timeout=30
                )

                if response.status_code == 200:
                    result = response.json()
                    if 'choices' in result and len(result['choices']) > 0:
                        content = result['choices'][0]['message']['content']
                        return content.strip()
                    else:
                        self.logger.warning(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                        return None
                else:
                    self.logger.warning(f"APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code}): {response.text}")

            except requests.RequestException as e:
                self.logger.warning(f"APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)

        self.logger.error("APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
        return None

class SmartArticleSummarizer:
    """æ™ºèƒ½æ–‡ç« æ€»ç»“å™¨ä¸»ç±»"""

    def __init__(self, app_id: str):
        self.web_reader = SmartWebReader()
        self.friday_client = FridayAIClient(app_id)
        self.logger = logging.getLogger(__name__)

    def load_articles_from_json(self, json_file: str) -> List[Dict]:
        """ä»JSONæ–‡ä»¶åŠ è½½æ–‡ç« æ•°æ®"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                articles = json.load(f)
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(articles)} ç¯‡æ–‡ç« ")
            return articles
        except Exception as e:
            self.logger.error(f"åŠ è½½æ–‡ç« æ•°æ®å¤±è´¥: {e}")
            return []

    def summarize_article_smart(self, article_data: Dict) -> Dict:
        """æ™ºèƒ½æ€»ç»“å•ç¯‡æ–‡ç« ï¼ˆå®æ—¶è·å–å†…å®¹ï¼‰"""
        self.logger.info(f"å¼€å§‹æ™ºèƒ½æ€»ç»“æ–‡ç« : {article_data.get('title', 'Unknown')}")

        # å®æ—¶è·å–æ–‡ç« å†…å®¹
        article_url = article_data.get('url', '')
        if article_url:
            realtime_content = self.web_reader.read_article_realtime(article_url)
        else:
            realtime_content = "æ— æ³•è·å–æ–‡ç« é“¾æ¥"

        # åˆ›å»ºæç¤ºè¯
        prompt = self.friday_client.create_summary_prompt(article_data, realtime_content)

        # è°ƒç”¨AIè¿›è¡Œæ€»ç»“
        summary = self.friday_client.call_friday_api(prompt)

        # æ„å»ºç»“æœ
        result = article_data.copy()
        result['ai_summary'] = summary or "æ€»ç»“ç”Ÿæˆå¤±è´¥"
        result['realtime_content_length'] = len(realtime_content)
        result['content_source'] = "realtime" if realtime_content != "æ— æ³•è·å–æ–‡ç« å†…å®¹" else "failed"
        result['summary_generated_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        return result

    def summarize_articles_smart(self, articles: List[Dict], batch_size: int = 3, delay: float = 3.0) -> List[Dict]:
        """æ™ºèƒ½æ‰¹é‡æ€»ç»“æ–‡ç« """
        summarized_articles = []
        total = len(articles)

        self.logger.info(f"å¼€å§‹æ™ºèƒ½æ€»ç»“ {total} ç¯‡æ–‡ç« ...")
        print(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ€»ç»“ {total} ç¯‡æ–‡ç« ...")
        print("ğŸ“– æ¯ç¯‡æ–‡ç« éƒ½ä¼šå®æ—¶è·å–æœ€æ–°å†…å®¹è¿›è¡Œåˆ†æ")

        for i, article in enumerate(articles, 1):
            try:
                print(f"\nğŸ“„ å¤„ç†è¿›åº¦: {i}/{total} - {article.get('title', 'Unknown')[:50]}...")

                # æ™ºèƒ½æ€»ç»“æ–‡ç« 
                summarized_article = self.summarize_article_smart(article)
                summarized_articles.append(summarized_article)

                # æ˜¾ç¤ºå¤„ç†ç»“æœ
                content_length = summarized_article.get('realtime_content_length', 0)
                content_source = summarized_article.get('content_source', 'unknown')

                if content_source == "realtime":
                    print(f"   âœ… æˆåŠŸè·å–å®æ—¶å†…å®¹ ({content_length} å­—ç¬¦)")
                else:
                    print(f"   âš ï¸ å†…å®¹è·å–å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯")

                # æ‰¹æ¬¡å»¶è¿Ÿ
                if i % batch_size == 0 and i < total:
                    print(f"   â³ æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç­‰å¾… {delay} ç§’...")
                    time.sleep(delay)
                else:
                    time.sleep(1)  # åŸºç¡€å»¶è¿Ÿ

            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ç« å¤±è´¥ ({i}/{total}): {e}")
                # æ·»åŠ å¤±è´¥çš„æ–‡ç« 
                failed_article = article.copy()
                failed_article['ai_summary'] = f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}"
                failed_article['realtime_content_length'] = 0
                failed_article['content_source'] = "error"
                failed_article['summary_generated_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                summarized_articles.append(failed_article)
                print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}")

        self.logger.info(f"æ™ºèƒ½æ€»ç»“å®Œæˆï¼æˆåŠŸå¤„ç† {len(summarized_articles)} ç¯‡æ–‡ç« ")
        return summarized_articles

    def save_smart_results(self, articles: List[Dict], output_file: Optional[str] = None):
        """ä¿å­˜æ™ºèƒ½æ€»ç»“ç»“æœ"""
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'smart_summarized_articles_{timestamp}.json'

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            self.logger.info(f"æ™ºèƒ½æ€»ç»“ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

    def generate_smart_report(self, articles: List[Dict], report_file: Optional[str] = None):
        """ç”Ÿæˆæ™ºèƒ½æ€»ç»“æŠ¥å‘Š"""
        if not report_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = f'smart_summary_report_{timestamp}.md'

        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# AIæ–‡ç« æ™ºèƒ½æ€»ç»“æŠ¥å‘Šï¼ˆå®æ—¶å†…å®¹ç‰ˆï¼‰\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ–‡ç« æ€»æ•°: {len(articles)}\n\n")

                # ç»Ÿè®¡ä¿¡æ¯
                categories = {}
                success_count = 0
                realtime_success = 0

                for article in articles:
                    category = article.get('category', 'æœªåˆ†ç±»')
                    categories[category] = categories.get(category, 0) + 1

                    if article.get('ai_summary') and not article['ai_summary'].startswith('æ€»ç»“ç”Ÿæˆå¤±è´¥'):
                        success_count += 1

                    if article.get('content_source') == 'realtime':
                        realtime_success += 1

                f.write(f"æˆåŠŸæ€»ç»“: {success_count} ç¯‡\n")
                f.write(f"å®æ—¶å†…å®¹è·å–æˆåŠŸ: {realtime_success} ç¯‡\n")
                f.write(f"å¤±è´¥æ•°é‡: {len(articles) - success_count} ç¯‡\n\n")

                f.write("## åˆ†ç±»ç»Ÿè®¡\n\n")
                for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"- {category}: {count} ç¯‡\n")

                f.write("\n## æ–‡ç« æ™ºèƒ½æ€»ç»“è¯¦æƒ…\n\n")

                for i, article in enumerate(articles, 1):
                    f.write(f"### {i}. {article.get('title', 'Unknown Title')}\n\n")
                    f.write(f"**åˆ†ç±»**: {article.get('category', 'æœªåˆ†ç±»')}\n")
                    f.write(f"**æ—¶é—´**: {article.get('publish_time', 'æœªçŸ¥')}\n")
                    f.write(f"**é“¾æ¥**: {article.get('url', '')}\n")
                    f.write(f"**å†…å®¹æ¥æº**: {'å®æ—¶è·å–' if article.get('content_source') == 'realtime' else 'åŸºæœ¬ä¿¡æ¯'}\n")
                    f.write(f"**å†…å®¹é•¿åº¦**: {article.get('realtime_content_length', 0)} å­—ç¬¦\n\n")

                    if article.get('ai_summary'):
                        f.write("**AIæ™ºèƒ½æ€»ç»“**:\n\n")
                        f.write(f"{article['ai_summary']}\n\n")
                    else:
                        f.write("**åŸå§‹æè¿°**:\n\n")
                        f.write(f"{article.get('description', 'æ— æè¿°')}\n\n")

                    f.write("---\n\n")

            self.logger.info(f"æ™ºèƒ½æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

        except Exception as e:
            self.logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

    def run_smart_summary(self, input_file: str, batch_size: int = 3, delay: float = 3.0):
        """è¿è¡Œæ™ºèƒ½æ€»ç»“æµç¨‹"""
        # åŠ è½½æ–‡ç« æ•°æ®
        articles = self.load_articles_from_json(input_file)
        if not articles:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°æ–‡ç« æ•°æ®")
            return []

        print(f"ğŸ“š åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« ")
        print("ğŸ”„ å°†ä¸ºæ¯ç¯‡æ–‡ç« å®æ—¶è·å–æœ€æ–°å†…å®¹è¿›è¡ŒAIæ€»ç»“")

        # æ™ºèƒ½æ€»ç»“æ–‡ç« 
        summarized_articles = self.summarize_articles_smart(articles, batch_size, delay)

        # ä¿å­˜ç»“æœ
        self.save_smart_results(summarized_articles)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_smart_report(summarized_articles)

        return summarized_articles

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    APP_ID = "21910615279495929878"  # ä½ çš„AppID
    INPUT_FILE = "../çˆ¬å–AIå’¨è¯¢/basic_articles_20250814_212345.json"  # åŸºæœ¬ä¿¡æ¯æ–‡ä»¶è·¯å¾„
    BATCH_SIZE = 2  # æ‰¹å¤„ç†å¤§å°ï¼ˆå®æ—¶è·å–å†…å®¹è¾ƒæ…¢ï¼Œå»ºè®®å‡å°ï¼‰
    DELAY = 4.0     # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆç§’ï¼‰

    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(INPUT_FILE):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_FILE}")
            print("ğŸ’¡ è¯·å…ˆä½¿ç”¨äº¤äº’å¼çˆ¬è™«è·å–åŸºæœ¬æ–‡ç« ä¿¡æ¯")
            print("   é€‰æ‹© '3. âš¡ ä»…è·å–åŸºæœ¬ä¿¡æ¯' é€‰é¡¹")
            return

        # åˆ›å»ºæ™ºèƒ½æ€»ç»“å™¨
        summarizer = SmartArticleSummarizer(APP_ID)

        # è¿è¡Œæ™ºèƒ½æ€»ç»“
        print("ğŸ§  å¯åŠ¨æ™ºèƒ½AIæ–‡ç« æ€»ç»“ç³»ç»Ÿ...")
        print("ğŸ“– ç‰¹ç‚¹ï¼šå®æ—¶è·å–ç½‘é¡µå†…å®¹ï¼Œç¡®ä¿åˆ†ææœ€æ–°ä¿¡æ¯")

        summarized_articles = summarizer.run_smart_summary(INPUT_FILE, BATCH_SIZE, DELAY)

        if summarized_articles:
            print(f"\nâœ… æ™ºèƒ½æ€»ç»“å®Œæˆï¼å…±å¤„ç† {len(summarized_articles)} ç¯‡æ–‡ç« ")
            print("ğŸ“ ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")

            # ç»Ÿè®¡æˆåŠŸç‡
            realtime_success = sum(1 for a in summarized_articles if a.get('content_source') == 'realtime')
            print(f"ğŸ“Š å®æ—¶å†…å®¹è·å–æˆåŠŸç‡: {realtime_success}/{len(summarized_articles)} ({realtime_success/len(summarized_articles)*100:.1f}%)")
        else:
            print("âŒ æ™ºèƒ½æ€»ç»“å¤±è´¥")

    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")

if __name__ == "__main__":
    main()
