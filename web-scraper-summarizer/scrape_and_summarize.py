#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘é¡µçˆ¬å–å’ŒAIæ€»ç»“ä¸€ä½“åŒ–å·¥å…·
ä¸€é”®å®Œæˆç½‘é¡µå†…å®¹çˆ¬å–å’Œæ™ºèƒ½æ€»ç»“
"""

import argparse
import sys
import os
import tempfile
from web_scraper import WebScraper
from ai_summarizer import AISummarizer
from friday_config import FRIDAY_CONFIG, setup_friday_env

def main():
    parser = argparse.ArgumentParser(description='ç½‘é¡µçˆ¬å–å’ŒAIæ€»ç»“ä¸€ä½“åŒ–å·¥å…·')
    parser.add_argument('url', help='è¦çˆ¬å–å’Œæ€»ç»“çš„ç½‘é¡µURL')

    # çˆ¬è™«å‚æ•°
    parser.add_argument('-t', '--timeout', type=int, default=10, help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰')

    # æ€»ç»“å‚æ•°
    parser.add_argument('--summary-type', choices=['comprehensive', 'brief', 'technical', 'academic'],
                       default='comprehensive', help='æ€»ç»“ç±»å‹')
    parser.add_argument('--model', help='ä½¿ç”¨çš„AIæ¨¡å‹åç§°')
    parser.add_argument('--api-type', choices=['friday', 'openai', 'local'], default='friday', help='APIç±»å‹')
    parser.add_argument('--api-key', help='APIå¯†é’¥ï¼ˆFridayä½¿ç”¨AppIdï¼‰')
    parser.add_argument('--base-url', help='APIåŸºç¡€URL')

    # è¾“å‡ºå‚æ•°
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--keep-scraped', action='store_true', help='ä¿ç•™çˆ¬å–çš„åŸå§‹å†…å®¹æ–‡ä»¶')

    args = parser.parse_args()

    # å¦‚æœä½¿ç”¨Friday APIä¸”æ²¡æœ‰æŒ‡å®šapi_keyï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„AppId
    if args.api_type == "friday" and not args.api_key:
        args.api_key = FRIDAY_CONFIG['app_id']
        if not args.base_url:
            args.base_url = FRIDAY_CONFIG['base_url']
        if not args.model:
            args.model = FRIDAY_CONFIG['default_model']

    print("ğŸš€ å¼€å§‹ç½‘é¡µçˆ¬å–å’ŒAIæ€»ç»“æµç¨‹...")
    print(f"ğŸŒ ç›®æ ‡URL: {args.url}")
    print(f"ğŸ¤– ä½¿ç”¨API: {args.api_type}")
    if args.api_type == "friday":
        print(f"ğŸ“± App ID: {args.api_key}")
        print(f"ğŸ¯ æ¨¡å‹: {args.model or FRIDAY_CONFIG['default_model']}")
    print("-" * 60)

    # ç¬¬ä¸€æ­¥ï¼šçˆ¬å–ç½‘é¡µå†…å®¹
    print("ğŸ“¥ ç¬¬ä¸€æ­¥ï¼šçˆ¬å–ç½‘é¡µå†…å®¹")

    # éªŒè¯URLæ ¼å¼
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'https://' + args.url

    scraper = WebScraper()

    # çˆ¬å–å†…å®¹
    html_content = scraper.fetch_content(args.url, args.timeout)
    if not html_content:
        print("âŒ ç½‘é¡µçˆ¬å–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)

    # æå–æ–‡æœ¬å†…å®¹
    content_data = scraper.extract_text_content(html_content, args.url)
    if not content_data:
        print("âŒ å†…å®¹æå–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)

    print(f"âœ… çˆ¬å–æˆåŠŸï¼")
    print(f"ğŸ“„ æ ‡é¢˜: {content_data['title']}")
    print(f"ğŸ“Š å†…å®¹é•¿åº¦: {content_data['length']} å­—ç¬¦")

    # ä¿å­˜çˆ¬å–å†…å®¹ï¼ˆä¸´æ—¶æ–‡ä»¶æˆ–æ°¸ä¹…æ–‡ä»¶ï¼‰
    if args.keep_scraped:
        scraped_filename = f"{args.output}_scraped.json" if args.output else None
        scraped_file = scraper.save_content(content_data, scraped_filename)
    else:
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
            import json
            json.dump(content_data, f, ensure_ascii=False, indent=2)
            scraped_file = f.name

    print("-" * 60)

    # ç¬¬äºŒæ­¥ï¼šAIæ€»ç»“
    print("ğŸ¤– ç¬¬äºŒæ­¥ï¼šAIæ™ºèƒ½æ€»ç»“")

    # åˆå§‹åŒ–æ€»ç»“å™¨
    summarizer = AISummarizer(
        api_type=args.api_type,
        api_key=args.api_key,
        base_url=args.base_url
    )

    # è¿›è¡Œæ€»ç»“
    summary_data = summarizer.summarize(content_data, args.summary_type, args.model)
    if not summary_data:
        print("âŒ AIæ€»ç»“å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if not args.keep_scraped and os.path.exists(scraped_file):
            os.unlink(scraped_file)
        sys.exit(1)

    print("âœ… æ€»ç»“å®Œæˆï¼")

    # æ˜¾ç¤ºæ€»ç»“ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“‹ AIæ€»ç»“ç»“æœ")
    print("="*60)
    print(summary_data['summary'])
    print("="*60)

    print(f"\nğŸ“ˆ å†…å®¹å‹ç¼©: {content_data['length']} â†’ {summary_data['summary_length']} å­—ç¬¦")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {summary_data['model_used']}")
    print(f"ğŸ“ æ€»ç»“ç±»å‹: {args.summary_type}")

    # ä¿å­˜æ€»ç»“ç»“æœ
    summary_filename = f"{args.output}_summary.json" if args.output else None
    saved_file = summarizer.save_summary(summary_data, summary_filename)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if not args.keep_scraped and os.path.exists(scraped_file):
        os.unlink(scraped_file)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    if args.keep_scraped:
        print(f"ğŸ“ åŸå§‹å†…å®¹: {scraped_file}")
    print(f"ğŸ“ æ€»ç»“ç»“æœ: {saved_file}")

    # æ˜¾ç¤ºä½¿ç”¨å»ºè®®
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print(f"   - å¦‚éœ€ä¸åŒç±»å‹çš„æ€»ç»“ï¼Œå¯ä½¿ç”¨: --summary-type brief|technical|academic")
    print(f"   - å¦‚éœ€ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œå¯ä½¿ç”¨: --model gpt-4")
    print(f"   - å¦‚éœ€ä¿ç•™åŸå§‹å†…å®¹ï¼Œå¯ä½¿ç”¨: --keep-scraped")

if __name__ == "__main__":
    main()
