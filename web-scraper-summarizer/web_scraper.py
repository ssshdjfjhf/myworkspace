#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘é¡µå†…å®¹çˆ¬å–å’ŒAIæ€»ç»“å·¥å…·
æ”¯æŒçˆ¬å–ç½‘é¡µå†…å®¹å¹¶ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½æ€»ç»“
"""

import requests
from bs4 import BeautifulSoup
import re
import sys
import argparse
from urllib.parse import urljoin, urlparse
import time
import json
from datetime import datetime

class WebScraper:
    def __init__(self):
        self.session = requests.Session()
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

    def fetch_content(self, url, timeout=10):
        """
        è·å–ç½‘é¡µå†…å®¹
        """
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç½‘é¡µè®¿é—®å¤±è´¥: {e}")
            return None

    def extract_text_content(self, html_content, url):
        """
        ä»HTMLä¸­æå–ä¸»è¦æ–‡æœ¬å†…å®¹
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
                tag.decompose()

            # è·å–æ ‡é¢˜
            title = ""
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()

            # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = None

            # å¸¸è§çš„ä¸»è¦å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'article', 'main', '.content', '.post-content', '.entry-content',
                '.article-content', '.post-body', '.content-body', '#content',
                '.main-content', '.article-body'
            ]

            for selector in content_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break

            # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
            if not main_content:
                main_content = soup.find('body')

            if not main_content:
                main_content = soup

            # æå–æ–‡æœ¬
            text_content = main_content.get_text(separator='\n', strip=True)

            # æ¸…ç†æ–‡æœ¬
            text_content = re.sub(r'\n\s*\n', '\n\n', text_content)  # åˆå¹¶å¤šä¸ªç©ºè¡Œ
            text_content = re.sub(r'[ \t]+', ' ', text_content)  # åˆå¹¶å¤šä¸ªç©ºæ ¼

            return {
                'title': title,
                'content': text_content,
                'url': url,
                'length': len(text_content),
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            print(f"âŒ å†…å®¹æå–å¤±è´¥: {e}")
            return None

    def save_content(self, content_data, filename=None):
        """
        ä¿å­˜çˆ¬å–çš„å†…å®¹åˆ°æ–‡ä»¶
        """
        if not filename:
            # æ ¹æ®URLç”Ÿæˆæ–‡ä»¶å
            parsed_url = urlparse(content_data['url'])
            domain = parsed_url.netloc.replace('.', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"scraped_content_{domain}_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(content_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return None

def main():
    parser = argparse.ArgumentParser(description='ç½‘é¡µå†…å®¹çˆ¬å–å·¥å…·')
    parser.add_argument('url', help='è¦çˆ¬å–çš„ç½‘é¡µURL')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('-t', '--timeout', type=int, default=10, help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰')

    args = parser.parse_args()

    # éªŒè¯URLæ ¼å¼
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'https://' + args.url

    scraper = WebScraper()

    # çˆ¬å–å†…å®¹
    html_content = scraper.fetch_content(args.url, args.timeout)
    if not html_content:
        sys.exit(1)

    # æå–æ–‡æœ¬å†…å®¹
    content_data = scraper.extract_text_content(html_content, args.url)
    if not content_data:
        sys.exit(1)

    print(f"ğŸ“„ æ ‡é¢˜: {content_data['title']}")
    print(f"ğŸ“Š å†…å®¹é•¿åº¦: {content_data['length']} å­—ç¬¦")

    # ä¿å­˜å†…å®¹
    saved_file = scraper.save_content(content_data, args.output)

    # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
    preview_length = 500
    if content_data['content']:
        print(f"\nğŸ“– å†…å®¹é¢„è§ˆ (å‰{preview_length}å­—ç¬¦):")
        print("-" * 50)
        print(content_data['content'][:preview_length])
        if len(content_data['content']) > preview_length:
            print("...")
        print("-" * 50)

    print(f"\nâœ… çˆ¬å–å®Œæˆï¼å†…å®¹å·²ä¿å­˜åˆ°: {saved_file}")
    print("ğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨AIæ€»ç»“å·¥å…·å¯¹å†…å®¹è¿›è¡Œæ€»ç»“")

if __name__ == "__main__":
    main()
