网页爬取和AI总结工具 使用说明

这是一个强大的网页内容爬取和AI智能总结工具，可以帮你快速获取网页内容并生成高质量的总结。

## 🚀 快速开始

### 1. 安装依赖
```bash
# 进入项目目录
cd web-scraper-summarizer

# 安装Python依赖
pip install -r requirements.txt
```

### 2. 配置API（已预配置美团Friday）
本工具已预配置美团Friday大模型，可直接使用：
```bash
# 直接使用（使用预配置的Friday API）
python scrape_and_summarize.py "https://example.com"
```

如果要使用其他API，可以设置环境变量：
```bash
# 使用OpenAI API
export OPENAI_API_KEY="your-api-key-here"
python scrape_and_summarize.py "https://example.com" --api-type openai

# 使用自定义Friday配置
export FRIDAY_APP_ID="your-app-id"
python scrape_and_summarize.py "https://example.com" --api-key "your-app-id"
```

### 3. 一键运行
```bash
# 最简单的用法（使用预配置的Friday）
python scrape_and_summarize.py "https://example.com"

# 使用不同的模型
python scrape_and_summarize.py "https://example.com" --model "LongCat-13B-128K-Chat"

# 使用OpenAI
python scrape_and_summarize.py "https://example.com" --api-type openai --api-key "your-openai-key"
```

## 📚 详细使用方法

### 主要工具

#### 1. 一体化工具（推荐）
`scrape_and_summarize.py` - 一键完成爬取和总结

```bash
# 基础用法
python scrape_and_summarize.py "网页URL"

# 完整参数示例
python scrape_and_summarize.py "https://example.com" \
    --summary-type comprehensive \
    --model gpt-3.5-turbo \
    --api-type openai \
    --api-key "your-key" \
    --output "my_summary" \
    --keep-scraped \
    --timeout 15
```

#### 2. 单独的爬虫工具
`web_scraper.py` - 只进行网页爬取

```bash
# 基础爬取
python web_scraper.py "https://example.com"

# 指定输出文件
python web_scraper.py "https://example.com" -o "scraped_content.json"
```

#### 3. 单独的总结工具
`ai_summarizer.py` - 对已爬取的内容进行总结

```bash
# 总结已保存的内容
python ai_summarizer.py "scraped_content.json"

# 指定总结类型
python ai_summarizer.py "scraped_content.json" --type technical
```

## 🎯 总结类型说明

- **comprehensive** (默认): 全面总结，包含概述、要点、重要信息等
- **brief**: 简要总结，2-3句话概括主要内容
- **technical**: 技术角度分析，提取技术要点和概念
- **academic**: 学术角度分析，关注核心观点和论证逻辑

## 🤖 支持的AI模型

### 美团Friday模型（默认）
- `LongCat-8B-128K-Chat` (默认，支持长文本)
- `LongCat-13B-128K-Chat` (更强性能)

### OpenAI模型
- `gpt-3.5-turbo` (性价比高)
- `gpt-4` (质量更高，成本更高)
- `gpt-4-turbo`

### 本地部署模型
支持兼容OpenAI API格式的本地模型：
```bash
python scrape_and_summarize.py "https://example.com" \
    --api-type local \
    --base-url "http://localhost:8000/v1" \
    --model "chatglm"
```

## 📝 使用示例

### 示例1：技术文章总结
```bash
python scrape_and_summarize.py "https://tech-blog.com/article" \
    --summary-type technical \
    --output "tech_summary"
```

### 示例2：新闻快速总结
```bash
python scrape_and_summarize.py "https://news.com/article" \
    --summary-type brief
```

### 示例3：学术论文分析
```bash
python scrape_and_summarize.py "https://arxiv.org/abs/xxxx" \
    --summary-type academic \
    --model gpt-4
```

### 示例4：保留原始内容
```bash
python scrape_and_summarize.py "https://example.com" \
    --keep-scraped \
    --output "complete_analysis"
```

## 🔧 高级配置

### 环境变量配置
```bash
# OpenAI配置
export OPENAI_API_KEY="your-key"
export OPENAI_BASE_URL="https://api.openai.com/v1"

# 本地模型配置
export LOCAL_MODEL_URL="http://localhost:8000/v1"
export DEFAULT_MODEL="chatglm"
```

### 批量处理脚本
创建一个批量处理多个URL的脚本：
```bash
#!/bin/bash
urls=(
    "https://example1.com"
    "https://example2.com"
    "https://example3.com"
)

for url in "${urls[@]}"; do
    echo "处理: $url"
    python scrape_and_summarize.py "$url" --output "batch_$(date +%s)"
done
```

## 📁 输出文件说明

### 爬取内容文件 (JSON格式)
```json
{
  "title": "网页标题",
  "content": "提取的文本内容",
  "url": "原始URL",
  "length": 12345,
  "timestamp": "2024-01-01T12:00:00"
}
```

### 总结结果文件 (JSON格式)
```json
{
  "original_title": "原文标题",
  "original_url": "原始URL",
  "summary_type": "comprehensive",
  "model_used": "gpt-3.5-turbo",
  "summary": "AI生成的总结内容",
  "original_length": 12345,
  "summary_length": 500,
  "timestamp": "2024-01-01T12:00:00"
}
```

## ⚠️ 注意事项

1. **API费用**: 使用OpenAI API会产生费用，建议先用gpt-3.5-turbo测试
2. **网站限制**: 某些网站可能有反爬虫机制，如遇到问题可调整请求头
3. **内容长度**: 超长内容可能会被截断，注意模型的token限制
4. **网络环境**: 确保网络连接稳定，可调整timeout参数

## 🆘 常见问题

### Q: 提示"模块未找到"错误
A: 请确保已安装所有依赖：`pip install -r requirements.txt`

### Q: API调用失败
A: 检查API密钥是否正确，网络是否正常，余额是否充足

### Q: 爬取失败
A: 可能是网站限制，尝试增加timeout或更换User-Agent

### Q: 总结质量不满意
A: 尝试不同的总结类型或更高级的模型（如gpt-4）

## 🎉 开始使用吧！

现在你可以开始使用这个强大的工具了。给我一个网页链接，我就能帮你快速获取内容并生成智能总结！

```bash
# 立即试试这个命令
python scrape_and_summarize.py "你的网页链接"
id