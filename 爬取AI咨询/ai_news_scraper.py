#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå·¥å…·é›†ç½‘ç«™çˆ¬è™«
ä¸“é—¨çˆ¬å–æœ€æ–°AIé¡¹ç›®èµ„è®¯
åŸºäºå®é™…HTMLç»“æ„ä¼˜åŒ–
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import os
from datetime import datetime
import logging
from urllib.parse import urljoin
import re

class AINewsScraper:
    def __init__(self):
        self.base_url = "https://ai-bot.cn"
        self.target_url = "https://ai-bot.cn/the-latest-ai-projects/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://ai-bot.cn/',
        })

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_page_content(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except requests.RequestException as e:
                self.logger.warning(f"è·å–é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    self.logger.error(f"è·å–é¡µé¢æœ€ç»ˆå¤±è´¥: {url}")
                    return None

    def parse_article_list(self, html_content):
        """è§£ææ–‡ç« åˆ—è¡¨ - åŸºäºå®é™…HTMLç»“æ„"""
        soup = BeautifulSoup(html_content, 'html.parser')
        articles = []

        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« å®¹å™¨ - åŸºäºæä¾›çš„HTMLç»“æ„
        article_containers = soup.find_all('div', class_='list-grid list-grid-padding')

        self.logger.info(f"æ‰¾åˆ° {len(article_containers)} ä¸ªæ–‡ç« å®¹å™¨")

        for container in article_containers:
            try:
                article_data = self.extract_article_info(container)
                if article_data:
                    articles.append(article_data)
            except Exception as e:
                self.logger.warning(f"è§£ææ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
                continue

        return articles

    def extract_article_info(self, container):
        """ä»å®¹å™¨ä¸­æå–æ–‡ç« ä¿¡æ¯ - åŸºäºå®é™…HTMLç»“æ„"""
        article = {}

        # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
        title_link = container.find('a', class_='list-title')
        if not title_link:
            return None

        # æå–æ ‡é¢˜ï¼ˆå»é™¤"æ–°"æ ‡ç­¾ï¼‰
        title_text = title_link.get_text(strip=True)
        # ç§»é™¤å¼€å¤´çš„"æ–°"å­—ç¬¦
        if title_text.startswith('æ–°'):
            title_text = title_text[1:].strip()

        article['title'] = title_text
        article['url'] = title_link.get('href', '')

        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
        if article['url'] and not article['url'].startswith('http'):
            article['url'] = urljoin(self.base_url, article['url'])

        # æŸ¥æ‰¾æè¿°
        desc_elem = container.find('div', class_='overflowClip_2')
        if desc_elem:
            article['description'] = desc_elem.get_text(strip=True)
        else:
            article['description'] = ''

        # æŸ¥æ‰¾åˆ†ç±»
        category_link = container.find('div', class_='list-footer').find('a')
        if category_link:
            article['category'] = category_link.get_text(strip=True)
        else:
            article['category'] = 'æœªåˆ†ç±»'

        # æŸ¥æ‰¾å‘å¸ƒæ—¶é—´
        time_elem = container.find('time')
        if time_elem:
            article['publish_time'] = time_elem.get_text(strip=True)
        else:
            article['publish_time'] = ''

        # æŸ¥æ‰¾å›¾ç‰‡é“¾æ¥
        img_elem = container.find('a', class_='media-content')
        if img_elem and img_elem.get('data-src'):
            article['image_url'] = img_elem.get('data-src')
        else:
            article['image_url'] = ''

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ–‡ç« 
        new_badge = container.find('span', class_='badge vc-red')
        article['is_new'] = bool(new_badge and 'æ–°' in new_badge.get_text())

        # æ·»åŠ çˆ¬å–æ—¶é—´
        article['scraped_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        return article

    def get_article_detail(self, article_url):
        """è·å–æ–‡ç« è¯¦ç»†å†…å®¹"""
        html_content = self.get_page_content(article_url)
        if not html_content:
            return None

        soup = BeautifulSoup(html_content, 'html.parser')

        # æŸ¥æ‰¾æ–‡ç« å†…å®¹
        content_selectors = [
            '.entry-content', '.post-content', '.article-content',
            '.content', 'article .content', '.main-content'
        ]

        content = ''
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in content_elem(["script", "style"]):
                    script.decompose()
                content = content_elem.get_text(strip=True)
                if len(content) > 100:  # ç¡®ä¿å†…å®¹æœ‰è¶³å¤Ÿé•¿åº¦
                    break

        return content

    def scrape_articles(self, max_pages=5, include_content=False):
        """çˆ¬å–æ–‡ç« """
        all_articles = []

        for page in range(1, max_pages + 1):
            if page == 1:
                url = self.target_url
            else:
                url = f"{self.target_url}page/{page}/"

            self.logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

            html_content = self.get_page_content(url)
            if not html_content:
                self.logger.warning(f"è·³è¿‡ç¬¬ {page} é¡µ")
                continue

            articles = self.parse_article_list(html_content)

            if not articles:
                self.logger.warning(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
                break

            self.logger.info(f"ç¬¬ {page} é¡µæ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

            # å¦‚æœéœ€è¦è·å–è¯¦ç»†å†…å®¹
            if include_content:
                for i, article in enumerate(articles):
                    self.logger.info(f"è·å–æ–‡ç« è¯¦ç»†å†…å®¹ ({i+1}/{len(articles)}): {article['title']}")
                    detail_content = self.get_article_detail(article['url'])
                    article['content'] = detail_content or ''
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

            all_articles.extend(articles)
            time.sleep(2)  # é¡µé¢é—´å»¶è¿Ÿ

        return all_articles

    def save_to_json(self, articles, filename='ai_articles.json'):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        self.logger.info(f"å·²ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ° {filename}")

    def save_to_csv(self, articles, filename='ai_articles.csv'):
        """ä¿å­˜ä¸ºCSVæ ¼å¼"""
        if not articles:
            return

        fieldnames = ['title', 'url', 'description', 'category', 'publish_time', 'image_url', 'is_new', 'scraped_at']
        if 'content' in articles[0]:
            fieldnames.append('content')

        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(articles)
        self.logger.info(f"å·²ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ° {filename}")

    def print_summary(self, articles):
        """æ‰“å°çˆ¬å–ç»“æœæ‘˜è¦"""
        if not articles:
            print("æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ–‡ç« ")
            return

        print(f"\n=== çˆ¬å–ç»“æœæ‘˜è¦ ===")
        print(f"æ€»æ–‡ç« æ•°: {len(articles)}")

        # ç»Ÿè®¡åˆ†ç±»
        categories = {}
        new_count = 0
        for article in articles:
            category = article.get('category', 'æœªåˆ†ç±»')
            categories[category] = categories.get(category, 0) + 1
            if article.get('is_new'):
                new_count += 1

        print(f"æ–°æ–‡ç« æ•°: {new_count}")
        print(f"åˆ†ç±»ç»Ÿè®¡:")
        for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            print(f"  {category}: {count}ç¯‡")

        print(f"\n=== æœ€æ–°æ–‡ç« ç¤ºä¾‹ ===")
        for i, article in enumerate(articles[:5]):
            status = "ğŸ†•" if article.get('is_new') else "ğŸ“„"
            print(f"\n{i+1}. {status} {article['title']}")
            print(f"   åˆ†ç±»: {article['category']}")
            print(f"   æ—¶é—´: {article['publish_time']}")
            print(f"   é“¾æ¥: {article['url']}")
            if article['description']:
                desc = article['description'][:100] + "..." if len(article['description']) > 100 else article['description']
                print(f"   æè¿°: {desc}")

    def run(self, max_pages=3, include_content=False, save_formats=['json', 'csv']):
        """è¿è¡Œçˆ¬è™«"""
        self.logger.info("å¼€å§‹çˆ¬å–AIå·¥å…·é›†ç½‘ç«™...")

        articles = self.scrape_articles(max_pages=max_pages, include_content=include_content)

        if not articles:
            self.logger.error("æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ–‡ç« ")
            return []

        self.logger.info(f"æ€»å…±çˆ¬å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        if 'json' in save_formats:
            self.save_to_json(articles, f'ai_articles_{timestamp}.json')

        if 'csv' in save_formats:
            self.save_to_csv(articles, f'ai_articles_{timestamp}.csv')

        # æ‰“å°æ‘˜è¦
        self.print_summary(articles)

        return articles

def main():
    """ä¸»å‡½æ•°"""
    scraper = AINewsScraper()

    # é…ç½®å‚æ•°
    MAX_PAGES = 3  # çˆ¬å–é¡µæ•°
    INCLUDE_CONTENT = False  # æ˜¯å¦åŒ…å«æ–‡ç« è¯¦ç»†å†…å®¹ï¼ˆä¼šæ˜¾è‘—å¢åŠ çˆ¬å–æ—¶é—´ï¼‰
    SAVE_FORMATS = ['json', 'csv']  # ä¿å­˜æ ¼å¼

    try:
        articles = scraper.run(
            max_pages=MAX_PAGES,
            include_content=INCLUDE_CONTENT,
            save_formats=SAVE_FORMATS
        )

        if articles:
            print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
            print("ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")

    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")

if __name__ == "__main__":
    main()
