#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆAIæ–‡ç« çˆ¬è™«
ä¸“é—¨ç”¨äºè·å–å®Œæ•´æ–‡ç« å†…å®¹ï¼Œä¸ºAIæ€»ç»“æä¾›æ›´ä¸°å¯Œçš„æ•°æ®
"""

import sys
import os
sys.path.append('.')

from ai_news_scraper import AINewsScraper
import json
import time
from datetime import datetime

class EnhancedAIScraper(AINewsScraper):
    """å¢å¼ºç‰ˆAIæ–‡ç« çˆ¬è™«"""

    def __init__(self):
        super().__init__()
        self.logger.info("åˆå§‹åŒ–å¢å¼ºç‰ˆAIæ–‡ç« çˆ¬è™«...")

    def scrape_with_full_content(self, max_pages=2, delay_between_articles=2):
        """çˆ¬å–æ–‡ç« å¹¶è·å–å®Œæ•´å†…å®¹"""
        self.logger.info(f"å¼€å§‹çˆ¬å– {max_pages} é¡µæ–‡ç« ï¼ŒåŒ…å«å®Œæ•´å†…å®¹...")

        # å…ˆè·å–æ–‡ç« åˆ—è¡¨
        articles = self.scrape_articles(max_pages=max_pages, include_content=False)

        if not articles:
            self.logger.error("æ²¡æœ‰è·å–åˆ°æ–‡ç« åˆ—è¡¨")
            return []

        self.logger.info(f"è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹è·å–è¯¦ç»†å†…å®¹...")

        # ä¸ºæ¯ç¯‡æ–‡ç« è·å–è¯¦ç»†å†…å®¹
        enhanced_articles = []
        total = len(articles)

        for i, article in enumerate(articles, 1):
            try:
                self.logger.info(f"å¤„ç†æ–‡ç«  {i}/{total}: {article['title']}")

                # è·å–æ–‡ç« è¯¦ç»†å†…å®¹
                detail_content = self.get_article_detail(article['url'])
                article['content'] = detail_content or article.get('description', '')

                # æ·»åŠ å†…å®¹ç»Ÿè®¡ä¿¡æ¯
                article['content_length'] = len(article['content'])
                article['has_full_content'] = len(article['content']) > 500

                enhanced_articles.append(article)

                # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i < total:
                    time.sleep(delay_between_articles)

            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ç« å¤±è´¥ {i}/{total}: {e}")
                # å³ä½¿å¤±è´¥ä¹Ÿä¿ç•™åŸºæœ¬ä¿¡æ¯
                article['content'] = article.get('description', '')
                article['content_length'] = len(article['content'])
                article['has_full_content'] = False
                enhanced_articles.append(article)

        return enhanced_articles

    def analyze_content_quality(self, articles):
        """åˆ†æå†…å®¹è´¨é‡"""
        if not articles:
            return

        total = len(articles)
        with_full_content = sum(1 for a in articles if a.get('has_full_content', False))
        avg_length = sum(a.get('content_length', 0) for a in articles) / total if total > 0 else 0

        self.logger.info(f"å†…å®¹è´¨é‡åˆ†æ:")
        self.logger.info(f"  æ€»æ–‡ç« æ•°: {total}")
        self.logger.info(f"  å®Œæ•´å†…å®¹: {with_full_content} ({with_full_content/total*100:.1f}%)")
        self.logger.info(f"  å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")

        print(f"\nğŸ“Š å†…å®¹è´¨é‡åˆ†æ:")
        print(f"  æ€»æ–‡ç« æ•°: {total}")
        print(f"  å®Œæ•´å†…å®¹: {with_full_content} ({with_full_content/total*100:.1f}%)")
        print(f"  å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")

    def save_enhanced_data(self, articles, filename_prefix="enhanced_articles"):
        """ä¿å­˜å¢å¼ºæ•°æ®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜JSONæ ¼å¼
        json_filename = f"{filename_prefix}_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

        self.logger.info(f"å¢å¼ºæ•°æ®å·²ä¿å­˜åˆ°: {json_filename}")

        # ç”Ÿæˆå†…å®¹é¢„è§ˆæŠ¥å‘Š
        report_filename = f"content_preview_{timestamp}.md"
        self.generate_content_preview(articles, report_filename)

        return json_filename

    def generate_content_preview(self, articles, filename):
        """ç”Ÿæˆå†…å®¹é¢„è§ˆæŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("# AIæ–‡ç« å†…å®¹é¢„è§ˆæŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ–‡ç« æ€»æ•°: {len(articles)}\n\n")

            for i, article in enumerate(articles, 1):
                f.write(f"## {i}. {article.get('title', 'Unknown Title')}\n\n")
                f.write(f"**åˆ†ç±»**: {article.get('category', 'æœªåˆ†ç±»')}\n")
                f.write(f"**æ—¶é—´**: {article.get('publish_time', 'æœªçŸ¥')}\n")
                f.write(f"**é“¾æ¥**: {article.get('url', '')}\n")
                f.write(f"**å†…å®¹é•¿åº¦**: {article.get('content_length', 0)} å­—ç¬¦\n")
                f.write(f"**å®Œæ•´å†…å®¹**: {'æ˜¯' if article.get('has_full_content', False) else 'å¦'}\n\n")

                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                content = article.get('content', '')
                if content:
                    preview = content[:300] + "..." if len(content) > 300 else content
                    f.write("**å†…å®¹é¢„è§ˆ**:\n")
                    f.write(f"```\n{preview}\n```\n\n")
                else:
                    f.write("**å†…å®¹é¢„è§ˆ**: æ— å†…å®¹\n\n")

                f.write("---\n\n")

        self.logger.info(f"å†…å®¹é¢„è§ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆAIæ–‡ç« çˆ¬è™«...")
    print("ğŸ“– æ­¤ç‰ˆæœ¬ä¼šè·å–å®Œæ•´æ–‡ç« å†…å®¹ï¼Œé€‚åˆAIæ€»ç»“åˆ†æ")

    scraper = EnhancedAIScraper()

    # é…ç½®å‚æ•°
    MAX_PAGES = 2  # å‡å°‘é¡µæ•°ï¼Œå› ä¸ºè¦è·å–å®Œæ•´å†…å®¹
    DELAY = 2      # æ–‡ç« é—´å»¶è¿Ÿï¼ˆç§’ï¼‰

    try:
        # çˆ¬å–æ–‡ç« 
        articles = scraper.scrape_with_full_content(
            max_pages=MAX_PAGES,
            delay_between_articles=DELAY
        )

        if articles:
            # åˆ†æå†…å®¹è´¨é‡
            scraper.analyze_content_quality(articles)

            # ä¿å­˜æ•°æ®
            filename = scraper.save_enhanced_data(articles)

            print(f"\nâœ… å¢å¼ºç‰ˆçˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {filename}")
            print(f"ğŸ“Š å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
            print(f"ğŸ’¡ å¯ä»¥ä½¿ç”¨æ­¤æ–‡ä»¶è¿›è¡ŒAIæ™ºèƒ½æ€»ç»“")

        else:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ–‡ç« æ•°æ®")

    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
