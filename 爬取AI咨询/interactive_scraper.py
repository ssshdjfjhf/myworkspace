#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº¤äº’å¼AIæ–‡ç« çˆ¬è™«
å…ˆé¢„è§ˆæ–‡ç« æ•°é‡ï¼Œè®©ç”¨æˆ·é€‰æ‹©çˆ¬å–ç­–ç•¥
"""

import sys
import os
sys.path.append('.')

from ai_news_scraper import AINewsScraper
import json
import time
from datetime import datetime

class InteractiveScraper(AINewsScraper):
    """äº¤äº’å¼AIæ–‡ç« çˆ¬è™«"""

    def __init__(self):
        super().__init__()
        self.logger.info("åˆå§‹åŒ–äº¤äº’å¼AIæ–‡ç« çˆ¬è™«...")

    def preview_articles(self, max_pages=5):
        """é¢„è§ˆæ–‡ç« ä¿¡æ¯ï¼Œä¸è·å–è¯¦ç»†å†…å®¹"""
        print("ğŸ” æ­£åœ¨é¢„è§ˆæ–‡ç« ä¿¡æ¯...")

        page_info = []
        total_articles = 0

        for page in range(1, max_pages + 1):
            if page == 1:
                url = self.target_url
            else:
                url = f"{self.target_url}page/{page}/"

            print(f"ğŸ“„ æ£€æŸ¥ç¬¬ {page} é¡µ...")

            html_content = self.get_page_content(url)
            if not html_content:
                print(f"âš ï¸ ç¬¬ {page} é¡µæ— æ³•è®¿é—®")
                break

            articles = self.parse_article_list(html_content)

            if not articles:
                print(f"âš ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
                break

            # ç»Ÿè®¡åˆ†ç±»
            categories = {}
            new_count = 0
            for article in articles:
                category = article.get('category', 'æœªåˆ†ç±»')
                categories[category] = categories.get(category, 0) + 1
                if article.get('is_new'):
                    new_count += 1

            page_info.append({
                'page': page,
                'article_count': len(articles),
                'new_count': new_count,
                'categories': categories,
                'articles': articles
            })

            total_articles += len(articles)
            print(f"   ğŸ“Š æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç«  (å…¶ä¸­ {new_count} ç¯‡æ–°æ–‡ç« )")

            time.sleep(1)  # é¢„è§ˆæ—¶ä¹Ÿè¦æ§åˆ¶é¢‘ç‡

        return page_info, total_articles

    def display_preview_summary(self, page_info, total_articles):
        """æ˜¾ç¤ºé¢„è§ˆæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ–‡ç« é¢„è§ˆæ‘˜è¦")
        print("="*60)

        print(f"ğŸ”¢ æ€»é¡µæ•°: {len(page_info)} é¡µ")
        print(f"ğŸ“° æ€»æ–‡ç« æ•°: {total_articles} ç¯‡")

        # ç»Ÿè®¡æ‰€æœ‰åˆ†ç±»
        all_categories = {}
        total_new = 0

        for page in page_info:
            total_new += page['new_count']
            for category, count in page['categories'].items():
                all_categories[category] = all_categories.get(category, 0) + count

        print(f"ğŸ†• æ–°æ–‡ç« æ•°: {total_new} ç¯‡")

        print(f"\nğŸ“‚ åˆ†ç±»ç»Ÿè®¡:")
        for category, count in sorted(all_categories.items(), key=lambda x: x[1], reverse=True):
            print(f"   {category}: {count} ç¯‡")

        print(f"\nğŸ“„ å„é¡µè¯¦æƒ…:")
        for page in page_info:
            print(f"   ç¬¬{page['page']}é¡µ: {page['article_count']}ç¯‡ (æ–°æ–‡ç« : {page['new_count']}ç¯‡)")

    def display_recent_articles(self, page_info, show_count=5):
        """æ˜¾ç¤ºæœ€æ–°æ–‡ç« æ ‡é¢˜"""
        print(f"\nğŸ“‹ æœ€æ–° {show_count} ç¯‡æ–‡ç« é¢„è§ˆ:")
        print("-" * 60)

        all_articles = []
        for page in page_info:
            all_articles.extend(page['articles'])

        for i, article in enumerate(all_articles[:show_count], 1):
            status = "ğŸ†•" if article.get('is_new') else "ğŸ“„"
            print(f"{i}. {status} {article['title']}")
            print(f"   ğŸ“‚ {article['category']} | â° {article['publish_time']}")
            if article['description']:
                desc = article['description'][:80] + "..." if len(article['description']) > 80 else article['description']
                print(f"   ğŸ’¬ {desc}")
            print()

    def get_user_choice(self, page_info, total_articles):
        """è·å–ç”¨æˆ·é€‰æ‹©"""
        print("\n" + "="*60)
        print("ğŸ¯ è¯·é€‰æ‹©çˆ¬å–ç­–ç•¥:")
        print("="*60)

        print("1. ğŸš€ çˆ¬å–æ‰€æœ‰é¡µé¢ (è·å–å®Œæ•´å†…å®¹)")
        print("2. ğŸ“ çˆ¬å–æŒ‡å®šé¡µæ•° (è·å–å®Œæ•´å†…å®¹)")
        print("3. âš¡ ä»…è·å–åŸºæœ¬ä¿¡æ¯ (ä¸è·å–å®Œæ•´å†…å®¹ï¼Œé€Ÿåº¦å¿«)")
        print("4. ğŸ” æŸ¥çœ‹æ›´å¤šæ–‡ç« é¢„è§ˆ")
        print("5. âŒ é€€å‡º")

        while True:
            try:
                choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()

                if choice == "1":
                    return "all", len(page_info), True
                elif choice == "2":
                    max_pages = len(page_info)
                    while True:
                        try:
                            pages = int(input(f"è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•° (1-{max_pages}): "))
                            if 1 <= pages <= max_pages:
                                return "custom", pages, True
                            else:
                                print(f"âŒ è¯·è¾“å…¥ 1-{max_pages} ä¹‹é—´çš„æ•°å­—")
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                elif choice == "3":
                    max_pages = len(page_info)
                    while True:
                        try:
                            pages = int(input(f"è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•° (1-{max_pages}): "))
                            if 1 <= pages <= max_pages:
                                return "basic", pages, False
                            else:
                                print(f"âŒ è¯·è¾“å…¥ 1-{max_pages} ä¹‹é—´çš„æ•°å­—")
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                elif choice == "4":
                    self.display_recent_articles(page_info, show_count=10)
                    continue
                elif choice == "5":
                    return "exit", 0, False
                else:
                    print("âŒ è¯·è¾“å…¥ 1-5 ä¹‹é—´çš„æ•°å­—")
            except KeyboardInterrupt:
                print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
                return "exit", 0, False

    def scrape_with_choice(self, pages, include_content=True):
        """æ ¹æ®ç”¨æˆ·é€‰æ‹©è¿›è¡Œçˆ¬å–"""
        if include_content:
            print(f"ğŸš€ å¼€å§‹çˆ¬å– {pages} é¡µæ–‡ç«  (åŒ…å«å®Œæ•´å†…å®¹)...")
            print("â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        else:
            print(f"âš¡ å¼€å§‹å¿«é€Ÿçˆ¬å– {pages} é¡µæ–‡ç«  (ä»…åŸºæœ¬ä¿¡æ¯)...")

        articles = self.scrape_articles(max_pages=pages, include_content=include_content)

        if articles and include_content:
            # åˆ†æå†…å®¹è´¨é‡
            with_content = sum(1 for a in articles if len(a.get('content', '')) > 500)
            avg_length = sum(len(a.get('content', '')) for a in articles) / len(articles)

            print(f"\nğŸ“Š å†…å®¹è´¨é‡åˆ†æ:")
            print(f"   å®Œæ•´å†…å®¹: {with_content}/{len(articles)} ç¯‡ ({with_content/len(articles)*100:.1f}%)")
            print(f"   å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")

        return articles

    def save_results(self, articles, strategy):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # æ ¹æ®ç­–ç•¥é€‰æ‹©æ–‡ä»¶åå‰ç¼€
        if strategy == "all":
            prefix = "full_articles"
        elif strategy == "custom":
            prefix = "custom_articles"
        elif strategy == "basic":
            prefix = "basic_articles"
        else:
            prefix = "articles"

        # ä¿å­˜JSONæ–‡ä»¶
        json_filename = f"{prefix}_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

        # ä¿å­˜CSVæ–‡ä»¶
        csv_filename = f"{prefix}_{timestamp}.csv"
        self.save_to_csv(articles, csv_filename)

        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜:")
        print(f"   ğŸ“„ JSON: {json_filename}")
        print(f"   ğŸ“Š CSV: {csv_filename}")

        return json_filename

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨äº¤äº’å¼AIæ–‡ç« çˆ¬è™«ï¼")
    print("ğŸ“– æ­¤å·¥å…·å¯ä»¥è®©ä½ å…ˆé¢„è§ˆæ–‡ç« ï¼Œå†å†³å®šçˆ¬å–ç­–ç•¥")

    scraper = InteractiveScraper()

    try:
        # ç¬¬ä¸€æ­¥ï¼šé¢„è§ˆæ–‡ç« 
        print("\nğŸ” ç¬¬ä¸€æ­¥ï¼šé¢„è§ˆæ–‡ç« ä¿¡æ¯")
        page_info, total_articles = scraper.preview_articles(max_pages=5)

        if not page_info:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return

        # ç¬¬äºŒæ­¥ï¼šæ˜¾ç¤ºé¢„è§ˆæ‘˜è¦
        scraper.display_preview_summary(page_info, total_articles)

        # ç¬¬ä¸‰æ­¥ï¼šæ˜¾ç¤ºæœ€æ–°æ–‡ç« 
        scraper.display_recent_articles(page_info)

        # ç¬¬å››æ­¥ï¼šè·å–ç”¨æˆ·é€‰æ‹©
        strategy, pages, include_content = scraper.get_user_choice(page_info, total_articles)

        if strategy == "exit":
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            return

        # ç¬¬äº”æ­¥ï¼šæ‰§è¡Œçˆ¬å–
        print(f"\nğŸš€ ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œçˆ¬å–ä»»åŠ¡")
        articles = scraper.scrape_with_choice(pages, include_content)

        if articles:
            # ç¬¬å…­æ­¥ï¼šä¿å­˜ç»“æœ
            print(f"\nğŸ’¾ ç¬¬ä¸‰æ­¥ï¼šä¿å­˜ç»“æœ")
            filename = scraper.save_results(articles, strategy)

            print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“Š å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {filename}")

            if include_content:
                print(f"ğŸ’¡ å¯ä»¥ä½¿ç”¨æ­¤æ–‡ä»¶è¿›è¡ŒAIæ™ºèƒ½æ€»ç»“")

        else:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ–‡ç« æ•°æ®")

    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ“ä½œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
